{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "filtered_bh.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOzmHj7ZxQ8uKtbEaKPQQzs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/22222bh/SketchTogether/blob/user%2FByeonghyeon/filtered_bh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSSGp-AdN1hi"
      },
      "source": [
        "초기 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nwTUYKTN0SA",
        "outputId": "fef4cc6c-6c5e-4dd1-db5a-6fd441f895c2"
      },
      "source": [
        "!pip install efficientnet_pytorch\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting efficientnet_pytorch\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet_pytorch) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch) (3.7.4.3)\n",
            "Building wheels for collected packages: efficientnet-pytorch\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=5efff18c26815390aac9cfa523d81a068cc3a5c5a748b2e473a1a8b3f0c034f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/cc/b2/49e74588263573ff778da58cc99b9c6349b496636a7e165be6\n",
            "Successfully built efficientnet-pytorch\n",
            "Installing collected packages: efficientnet-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1\n",
            "Collecting umap-learn\n",
            "  Downloading umap-learn-0.5.1.tar.gz (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 3.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.4.1)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.51.2)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.4.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 9.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (57.2.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pynndescent>=0.5->umap-learn) (1.0.1)\n",
            "Building wheels for collected packages: umap-learn, pynndescent\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.1-py3-none-any.whl size=76566 sha256=7fbcf2fb2d7b5b0d5448edfed98ee4821379f46eb7519dc0b936ec0d68ec05ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/e7/bb/347dc0e510803d7116a13d592b10cc68262da56a8eec4dd72f\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.4-py3-none-any.whl size=52372 sha256=dece4aeb2e8db3ba70caa5e079e8946fc23e86bc7657c5aba9bab4bd333ce70b\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/5b/62/3401692ddad12324249c774c4b15ccb046946021e2b581c043\n",
            "Successfully built umap-learn pynndescent\n",
            "Installing collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.4 umap-learn-0.5.1\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_vfG8EQN1VP"
      },
      "source": [
        "from efficientnet_pytorch import EfficientNet\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "# from keras.applications.vgg16 import VGG16 \n",
        "# from keras.models import Model\n",
        "# from keras.applications.vgg16 import preprocess_input \n",
        "import os\n",
        "import logging\n",
        "import torch\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Pyjvf2NODmd"
      },
      "source": [
        "filename = 'filtered_10'\n",
        "data_path = '/content/drive/MyDrive/Sketch_RNN/Dataset/Filtered_img/'\n",
        "cluster_list_path = '/content/drive/MyDrive/Sketch_RNN/Dataset/Filtered_img/cluster_list.txt'\n",
        "feat_path = \"/content/drive/MyDrive/skku_Clustering/features_hw_\" + filename + \".npy\"\n",
        "filename_path = \"/content/drive/MyDrive/skku_Clustering/filenames_hw_\" + filename + \".npy\"\n",
        "save_path = '/content/drive/MyDrive/skku_Clustering/Result/'\n",
        "test_path = '/content/drive/MyDrive/skku_Clustering/test_set/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JETaCQloN-s7"
      },
      "source": [
        "os.chdir(data_path)\n",
        "data_list = []\n",
        "with os.scandir(data_path) as files:\n",
        "    for file in files:\n",
        "        if not file.name.endswith('.txt'):\n",
        "            data_list.append(file.name)\n",
        "\n",
        "cluster_list = open(cluster_list_path , 'r').read().split('\\n') \n",
        "\n",
        "#transform\n",
        "tfms = transforms.Compose([transforms.Resize(224), transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),])\n",
        "\n",
        "def feature_extraction(image, model):\n",
        "    img = tfms(image.convert(\"RGB\")).unsqueeze(0)\n",
        "    features = model.extract_features(img)\n",
        "    return features\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF2O8Z0l8ALu"
      },
      "source": [
        "# Test set 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hF12WLlb7-4S",
        "outputId": "8a41d71e-d5e2-4364-a6af-3877abc426f5"
      },
      "source": [
        "cluster_list = open(cluster_list_path, 'r').read().split('\\n')\n",
        "\n",
        "s = 2000\n",
        "e = 2050\n",
        "for label in cluster_list:\n",
        "  path = test_path + label + '/'\n",
        "  print(path)\n",
        "  if os.path.isdir(path) == False:\n",
        "    os.makedirs(path)\n",
        "  os.chdir(path)\n",
        "  images = np.load('/content/drive/MyDrive/Sketch_RNN/Dataset/test/' + label + '.npy')\n",
        "  for i in range (s, e): \n",
        "    image = Image.fromarray(images[i].reshape(28, 28))\n",
        "    image.save('test_' + label + '_' + str(i - s + 1) + '.jpg')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/skku_Clustering/test_set/cat/\n",
            "/content/drive/MyDrive/skku_Clustering/test_set/circle/\n",
            "/content/drive/MyDrive/skku_Clustering/test_set/cloud/\n",
            "/content/drive/MyDrive/skku_Clustering/test_set/dog/\n",
            "/content/drive/MyDrive/skku_Clustering/test_set/ear/\n",
            "/content/drive/MyDrive/skku_Clustering/test_set/eye/\n",
            "/content/drive/MyDrive/skku_Clustering/test_set/moustache/\n",
            "/content/drive/MyDrive/skku_Clustering/test_set/mouth/\n",
            "/content/drive/MyDrive/skku_Clustering/test_set/nose/\n",
            "/content/drive/MyDrive/skku_Clustering/test_set/triangle/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdWegNuUHTu-"
      },
      "source": [
        "# Clustering algorithm 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nlxINqJC5Cj"
      },
      "source": [
        "_eps = 1.3\n",
        "_ms = 2\n",
        "\n",
        "def clustering_method(cmd, cluster_num, input):\n",
        "  if cmd == 'kmeans':\n",
        "    from sklearn.cluster import KMeans\n",
        "    kmeans = KMeans(n_clusters=cluster_num, n_init = 20, n_jobs=-1, random_state = 0)\n",
        "    kmeans.fit(input)\n",
        "    return kmeans.labels_\n",
        "\n",
        "  elif cmd == 'minibatch':\n",
        "    from sklearn.cluster import MiniBatchKMeans\n",
        "    mb_kmeans = MiniBatchKMeans(n_clusters=cluster_num,random_state=0, batch_size=6)\n",
        "    mb_kmeans.fit(input)\n",
        "    return mb_kmeans.labels_\n",
        "\n",
        "  elif cmd == 'dbscan':\n",
        "    from sklearn.cluster import DBSCAN\n",
        "    dbscan = DBSCAN(eps=_eps, min_samples=_ms) # eps 1.8이면 63프로\n",
        "    dbscan.fit(input)\n",
        "    return dbscan.labels_\n",
        "  \n",
        "  elif cmd == 'optics':\n",
        "    from sklearn.cluster import OPTICS, cluster_optics_dbscan\n",
        "    optics = OPTICS(min_samples = 5)\n",
        "    optics.fit(input)\n",
        "    return optics.labels_"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wupHmO777KFr",
        "outputId": "b794257c-c129-471a-a3ce-7c6da0f193a0"
      },
      "source": [
        "from torch import nn\n",
        "model = EfficientNet.from_pretrained('efficientnet-b0')\n",
        "data = {}\n",
        "noise = 0\n",
        "correct = 0\n",
        "wrong = 0\n",
        "avg_pooling = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "os.chdir(test_path)\n",
        "test_list = []\n",
        "with os.scandir(test_path) as files:\n",
        "  for file in files:\n",
        "    if not file.name.startswith('.'):\n",
        "      test_list.append(file.name)\n",
        "\n",
        "for folder in test_list:\n",
        "  test_images = os.listdir(test_path + folder)\n",
        "  for i, image_name in enumerate(test_images):\n",
        "    # train data의 feature vector load\n",
        "    feat = np.load(feat_path)\n",
        "\n",
        "    # test data에 대하여 feature 추출\n",
        "    image = Image.open(test_path + folder +'/'+ image_name)\n",
        "    new_feat = feature_extraction(image, model)\n",
        "    new_feat = avg_pooling(new_feat)\n",
        "    new_feat = new_feat.detach().numpy().reshape(-1)\n",
        "    new_feat = new_feat.reshape(1, -1)\n",
        "\n",
        "    # 기존 feature vector에 append. \n",
        "    feat = np.append(feat, new_feat, axis = 0)\n",
        "    # print(feat.shape) # 608이 나와야 함\n",
        "\n",
        "    # 차원 축소(t-SNE)\n",
        "    feat = tsne.fit_transform(feat)\n",
        "\n",
        "    # clustering\n",
        "    labels = clustering_method('dbscan', len(cluster_list), feat)\n",
        "\n",
        "    # 결과 보는 단계\n",
        "    filenames = np.load(filename_path)\n",
        "    filenames = np.append(filenames,image_name)\n",
        "\n",
        "    groups = {}\n",
        "    # holds the cluster id and the images { id: [images] }\n",
        "    i = 0\n",
        "    for f, cluster in zip(filenames, labels):\n",
        "        if cluster not in groups.keys():\n",
        "            groups[cluster] = []\n",
        "            groups[cluster].append(f)\n",
        "        else:\n",
        "            groups[cluster].append(f)\n",
        "\n",
        "\n",
        "    #make cluster_dict for calculate acc\n",
        "    cluster_dict = {}\n",
        "    for cluster in groups:\n",
        "      image_count = []\n",
        "      image_name = []\n",
        "      for image in groups[cluster]:\n",
        "        image_name.append(image.split('_')[0])\n",
        "      for name in cluster_list:\n",
        "        image_count.append(image_name.count(name))\n",
        "      cluster_dict[cluster] = cluster_list[image_count.index(max(image_count))]\n",
        "    \n",
        "    #Acc\n",
        "    from sklearn.metrics import f1_score\n",
        "    pred = []\n",
        "    gt = []    \n",
        "    for cluster in groups:\n",
        "        for food in groups[cluster]:\n",
        "            pred.append(cluster_dict[cluster])\n",
        "            gt.append(food.split('_')[0])\n",
        "            if food.split('_')[0] == 'test':\n",
        "              tmp = food.split('_')[1] + food.split('_')[2].split('.')[0]\n",
        "              if cluster == -1:\n",
        "                res = food.split('_')[1] + ' = ' + str(cluster)\n",
        "                noise+=1\n",
        "              else:\n",
        "                res = food.split('_')[1] + ' = ' + cluster_dict[cluster]\n",
        "                if food.split('_')[1] == cluster_dict[cluster]:\n",
        "                  correct += 1\n",
        "                else:\n",
        "                  wrong += 1\n",
        "\n",
        "    acc = str(round(f1_score(gt, pred,average='micro') * 100, 2))\n",
        "    print(tmp + ' ->  res: ' + res + '    ACC: ' + acc + '    #cluster: ' + str(len(cluster_dict)))\n",
        "    print('\\t\\t\\t', end='')\n",
        "    print(cluster_dict)\n",
        "\n",
        "print('-----result-----')\n",
        "print('total test: ', wrong + noise + correct)\n",
        "print('noise: ', noise)\n",
        "print('correct: ', correct)\n",
        "print('wrong: ', wrong)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded pretrained weights for efficientnet-b0\n",
            "cat3 ->  res: cat = cat    ACC: 78.45    #cluster: 110\n",
            "\t\t\t{0: 'ear', 1: 'ear', 2: 'ear', 3: 'ear', 4: 'ear', 5: 'ear', -1: 'cat', 6: 'ear', 7: 'dog', 8: 'ear', 9: 'ear', 10: 'ear', 11: 'circle', 12: 'circle', 13: 'cat', 14: 'cat', 15: 'cat', 16: 'cat', 17: 'cat', 18: 'cat', 19: 'cat', 20: 'cat', 21: 'cat', 22: 'cat', 23: 'cat', 24: 'cat', 25: 'cat', 26: 'cat', 27: 'cat', 28: 'cat', 29: 'cat', 30: 'cat', 31: 'nose', 32: 'cat', 33: 'dog', 34: 'cat', 35: 'dog', 36: 'cat', 37: 'cat', 38: 'cat', 39: 'cat', 40: 'cat', 41: 'dog', 42: 'dog', 43: 'dog', 44: 'dog', 45: 'dog', 46: 'cloud', 47: 'mouth', 48: 'dog', 49: 'dog', 50: 'dog', 51: 'dog', 52: 'eye', 53: 'dog', 54: 'dog', 55: 'dog', 56: 'dog', 57: 'dog', 58: 'dog', 59: 'nose', 60: 'nose', 61: 'nose', 62: 'nose', 63: 'triangle', 64: 'nose', 65: 'nose', 66: 'nose', 67: 'nose', 68: 'triangle', 69: 'triangle', 70: 'cloud', 71: 'moustache', 72: 'moustache', 73: 'moustache', 74: 'moustache', 75: 'cloud', 76: 'moustache', 77: 'cloud', 78: 'triangle', 79: 'triangle', 80: 'triangle', 81: 'triangle', 82: 'triangle', 83: 'triangle', 84: 'triangle', 85: 'triangle', 86: 'triangle', 87: 'triangle', 88: 'triangle', 89: 'cloud', 90: 'cloud', 91: 'cloud', 92: 'cloud', 93: 'cloud', 94: 'cloud', 95: 'cloud', 96: 'mouth', 97: 'cloud', 98: 'eye', 99: 'eye', 100: 'eye', 101: 'eye', 102: 'eye', 103: 'eye', 104: 'mouth', 105: 'mouth', 106: 'mouth', 107: 'mouth', 108: 'mouth'}\n",
            "cat5 ->  res: cat = -1    ACC: 78.29    #cluster: 108\n",
            "\t\t\t{0: 'ear', 1: 'ear', 2: 'ear', 3: 'ear', 4: 'ear', 5: 'ear', -1: 'dog', 6: 'dog', 7: 'ear', 8: 'ear', 9: 'ear', 10: 'circle', 11: 'circle', 12: 'cat', 13: 'cat', 14: 'cat', 15: 'cat', 16: 'cat', 17: 'cat', 18: 'cat', 19: 'cat', 20: 'cat', 21: 'cat', 22: 'cat', 23: 'cat', 24: 'cat', 25: 'cat', 26: 'cat', 27: 'nose', 28: 'cat', 29: 'cat', 30: 'cat', 31: 'cat', 32: 'dog', 33: 'cat', 34: 'cat', 35: 'cat', 36: 'cat', 37: 'dog', 38: 'dog', 39: 'dog', 40: 'dog', 41: 'cloud', 42: 'dog', 43: 'dog', 44: 'dog', 45: 'dog', 46: 'dog', 47: 'dog', 48: 'eye', 49: 'dog', 50: 'dog', 51: 'dog', 52: 'dog', 53: 'dog', 54: 'dog', 55: 'nose', 56: 'nose', 57: 'nose', 58: 'nose', 59: 'nose', 60: 'nose', 61: 'nose', 62: 'nose', 63: 'nose', 64: 'nose', 65: 'cloud', 66: 'moustache', 67: 'moustache', 68: 'moustache', 69: 'moustache', 70: 'cloud', 71: 'moustache', 72: 'eye', 73: 'triangle', 74: 'triangle', 75: 'triangle', 76: 'triangle', 77: 'triangle', 78: 'triangle', 79: 'triangle', 80: 'mouth', 81: 'triangle', 82: 'triangle', 83: 'triangle', 84: 'triangle', 85: 'triangle', 86: 'triangle', 87: 'cloud', 88: 'cloud', 89: 'cloud', 90: 'cloud', 91: 'cloud', 92: 'cloud', 93: 'cloud', 94: 'cloud', 95: 'cloud', 96: 'eye', 97: 'eye', 98: 'eye', 99: 'eye', 100: 'eye', 101: 'eye', 102: 'mouth', 103: 'mouth', 104: 'mouth', 105: 'mouth', 106: 'mouth'}\n",
            "cat7 ->  res: cat = cat    ACC: 79.61    #cluster: 104\n",
            "\t\t\t{0: 'ear', 1: 'ear', 2: 'ear', 3: 'ear', 4: 'ear', 5: 'ear', -1: 'dog', 6: 'dog', 7: 'ear', 8: 'ear', 9: 'ear', 10: 'circle', 11: 'dog', 12: 'cat', 13: 'cat', 14: 'cat', 15: 'cat', 16: 'cat', 17: 'dog', 18: 'cat', 19: 'cat', 20: 'cat', 21: 'cat', 22: 'cat', 23: 'cat', 24: 'cat', 25: 'cat', 26: 'cat', 27: 'cat', 28: 'cat', 29: 'dog', 30: 'cat', 31: 'cat', 32: 'cat', 33: 'cat', 34: 'dog', 35: 'dog', 36: 'dog', 37: 'cloud', 38: 'dog', 39: 'dog', 40: 'dog', 41: 'dog', 42: 'dog', 43: 'dog', 44: 'eye', 45: 'dog', 46: 'dog', 47: 'dog', 48: 'dog', 49: 'dog', 50: 'dog', 51: 'nose', 52: 'nose', 53: 'nose', 54: 'nose', 55: 'nose', 56: 'nose', 57: 'nose', 58: 'nose', 59: 'nose', 60: 'triangle', 61: 'cloud', 62: 'moustache', 63: 'moustache', 64: 'moustache', 65: 'cloud', 66: 'moustache', 67: 'cloud', 68: 'triangle', 69: 'triangle', 70: 'cloud', 71: 'triangle', 72: 'triangle', 73: 'triangle', 74: 'triangle', 75: 'mouth', 76: 'triangle', 77: 'triangle', 78: 'triangle', 79: 'triangle', 80: 'triangle', 81: 'cloud', 82: 'cloud', 83: 'cloud', 84: 'cloud', 85: 'cloud', 86: 'cloud', 87: 'cloud', 88: 'cloud', 89: 'cloud', 90: 'cloud', 91: 'eye', 92: 'eye', 93: 'eye', 94: 'eye', 95: 'eye', 96: 'eye', 97: 'mouth', 98: 'mouth', 99: 'mouth', 100: 'mouth', 101: 'mouth', 102: 'mouth'}\n",
            "cat8 ->  res: cat = cat    ACC: 80.59    #cluster: 107\n",
            "\t\t\t{0: 'ear', 1: 'ear', 2: 'ear', 3: 'ear', 4: 'ear', -1: 'dog', 5: 'dog', 6: 'ear', 7: 'ear', 8: 'ear', 9: 'circle', 10: 'circle', 11: 'cat', 12: 'cat', 13: 'cat', 14: 'cat', 15: 'cat', 16: 'cat', 17: 'dog', 18: 'cat', 19: 'cat', 20: 'cat', 21: 'cat', 22: 'cat', 23: 'cat', 24: 'cat', 25: 'cat', 26: 'cat', 27: 'cat', 28: 'cat', 29: 'nose', 30: 'cat', 31: 'cat', 32: 'cat', 33: 'cat', 34: 'dog', 35: 'cat', 36: 'cat', 37: 'cat', 38: 'dog', 39: 'dog', 40: 'dog', 41: 'dog', 42: 'cloud', 43: 'dog', 44: 'dog', 45: 'dog', 46: 'dog', 47: 'eye', 48: 'dog', 49: 'dog', 50: 'dog', 51: 'dog', 52: 'dog', 53: 'dog', 54: 'nose', 55: 'nose', 56: 'nose', 57: 'nose', 58: 'nose', 59: 'nose', 60: 'nose', 61: 'nose', 62: 'triangle', 63: 'cloud', 64: 'moustache', 65: 'moustache', 66: 'moustache', 67: 'cloud', 68: 'moustache', 69: 'eye', 70: 'cloud', 71: 'triangle', 72: 'triangle', 73: 'cloud', 74: 'triangle', 75: 'triangle', 76: 'triangle', 77: 'triangle', 78: 'mouth', 79: 'triangle', 80: 'triangle', 81: 'triangle', 82: 'triangle', 83: 'cloud', 84: 'cloud', 85: 'cloud', 86: 'cloud', 87: 'cloud', 88: 'cloud', 89: 'cloud', 90: 'cloud', 91: 'cloud', 92: 'cloud', 93: 'cloud', 94: 'cloud', 95: 'eye', 96: 'eye', 97: 'eye', 98: 'eye', 99: 'eye', 100: 'eye', 101: 'mouth', 102: 'mouth', 103: 'mouth', 104: 'mouth', 105: 'mouth'}\n",
            "cat9 ->  res: cat = cat    ACC: 78.62    #cluster: 106\n",
            "\t\t\t{0: 'ear', 1: 'ear', 2: 'ear', 3: 'ear', 4: 'ear', 5: 'ear', -1: 'dog', 6: 'dog', 7: 'ear', 8: 'ear', 9: 'ear', 10: 'circle', 11: 'circle', 12: 'dog', 13: 'cat', 14: 'cat', 15: 'cat', 16: 'cat', 17: 'dog', 18: 'cat', 19: 'cat', 20: 'cat', 21: 'cat', 22: 'cat', 23: 'cat', 24: 'cat', 25: 'cat', 26: 'cat', 27: 'cat', 28: 'cat', 29: 'cat', 30: 'nose', 31: 'cat', 32: 'cat', 33: 'dog', 34: 'dog', 35: 'cat', 36: 'cat', 37: 'cat', 38: 'cat', 39: 'dog', 40: 'dog', 41: 'dog', 42: 'dog', 43: 'cloud', 44: 'dog', 45: 'dog', 46: 'dog', 47: 'dog', 48: 'dog', 49: 'eye', 50: 'dog', 51: 'dog', 52: 'cloud', 53: 'dog', 54: 'dog', 55: 'nose', 56: 'nose', 57: 'nose', 58: 'nose', 59: 'nose', 60: 'nose', 61: 'nose', 62: 'nose', 63: 'nose', 64: 'triangle', 65: 'cloud', 66: 'moustache', 67: 'moustache', 68: 'moustache', 69: 'moustache', 70: 'cloud', 71: 'moustache', 72: 'cloud', 73: 'triangle', 74: 'cloud', 75: 'triangle', 76: 'triangle', 77: 'triangle', 78: 'triangle', 79: 'triangle', 80: 'triangle', 81: 'triangle', 82: 'triangle', 83: 'triangle', 84: 'triangle', 85: 'cloud', 86: 'cloud', 87: 'cloud', 88: 'cloud', 89: 'cloud', 90: 'cloud', 91: 'mouth', 92: 'cloud', 93: 'eye', 94: 'eye', 95: 'eye', 96: 'eye', 97: 'eye', 98: 'mouth', 99: 'mouth', 100: 'mouth', 101: 'mouth', 102: 'mouth', 103: 'mouth', 104: 'mouth'}\n",
            "cat10 ->  res: cat = cat    ACC: 78.95    #cluster: 96\n",
            "\t\t\t{0: 'ear', 1: 'ear', 2: 'ear', 3: 'ear', 4: 'ear', -1: 'cat', 5: 'dog', 6: 'ear', 7: 'ear', 8: 'ear', 9: 'circle', 10: 'dog', 11: 'cat', 12: 'cat', 13: 'cat', 14: 'dog', 15: 'cat', 16: 'cat', 17: 'cat', 18: 'cat', 19: 'cat', 20: 'cat', 21: 'cat', 22: 'cat', 23: 'cat', 24: 'nose', 25: 'cat', 26: 'dog', 27: 'cat', 28: 'cat', 29: 'cat', 30: 'cat', 31: 'cat', 32: 'dog', 33: 'dog', 34: 'dog', 35: 'dog', 36: 'dog', 37: 'dog', 38: 'cloud', 39: 'dog', 40: 'dog', 41: 'dog', 42: 'dog', 43: 'dog', 44: 'eye', 45: 'dog', 46: 'dog', 47: 'dog', 48: 'dog', 49: 'nose', 50: 'nose', 51: 'nose', 52: 'nose', 53: 'triangle', 54: 'nose', 55: 'nose', 56: 'nose', 57: 'nose', 58: 'triangle', 59: 'triangle', 60: 'cloud', 61: 'moustache', 62: 'moustache', 63: 'moustache', 64: 'cloud', 65: 'moustache', 66: 'triangle', 67: 'triangle', 68: 'triangle', 69: 'triangle', 70: 'triangle', 71: 'triangle', 72: 'triangle', 73: 'triangle', 74: 'cloud', 75: 'cloud', 76: 'cloud', 77: 'cloud', 78: 'cloud', 79: 'cloud', 80: 'cloud', 81: 'cloud', 82: 'cloud', 83: 'mouth', 84: 'cloud', 85: 'eye', 86: 'eye', 87: 'eye', 88: 'eye', 89: 'mouth', 90: 'mouth', 91: 'mouth', 92: 'mouth', 93: 'mouth', 94: 'mouth'}\n",
            "cat11 ->  res: cat = cat    ACC: 77.96    #cluster: 100\n",
            "\t\t\t{0: 'ear', 1: 'ear', 2: 'ear', 3: 'ear', 4: 'ear', 5: 'ear', -1: 'dog', 6: 'dog', 7: 'ear', 8: 'ear', 9: 'ear', 10: 'circle', 11: 'circle', 12: 'dog', 13: 'cat', 14: 'cat', 15: 'cat', 16: 'cat', 17: 'cat', 18: 'cat', 19: 'cat', 20: 'cat', 21: 'cat', 22: 'cat', 23: 'cat', 24: 'cat', 25: 'cat', 26: 'cat', 27: 'nose', 28: 'cat', 29: 'cat', 30: 'dog', 31: 'cat', 32: 'cat', 33: 'cat', 34: 'cat', 35: 'cat', 36: 'cat', 37: 'dog', 38: 'dog', 39: 'dog', 40: 'cloud', 41: 'dog', 42: 'dog', 43: 'dog', 44: 'dog', 45: 'dog', 46: 'eye', 47: 'dog', 48: 'dog', 49: 'dog', 50: 'dog', 51: 'nose', 52: 'nose', 53: 'nose', 54: 'nose', 55: 'triangle', 56: 'nose', 57: 'nose', 58: 'nose', 59: 'nose', 60: 'cloud', 61: 'moustache', 62: 'moustache', 63: 'moustache', 64: 'cloud', 65: 'moustache', 66: 'triangle', 67: 'triangle', 68: 'triangle', 69: 'triangle', 70: 'triangle', 71: 'triangle', 72: 'triangle', 73: 'triangle', 74: 'triangle', 75: 'triangle', 76: 'triangle', 77: 'cloud', 78: 'cloud', 79: 'cloud', 80: 'cloud', 81: 'cloud', 82: 'cloud', 83: 'cloud', 84: 'cloud', 85: 'mouth', 86: 'cloud', 87: 'eye', 88: 'eye', 89: 'eye', 90: 'eye', 91: 'eye', 92: 'eye', 93: 'mouth', 94: 'mouth', 95: 'mouth', 96: 'mouth', 97: 'mouth', 98: 'mouth'}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}