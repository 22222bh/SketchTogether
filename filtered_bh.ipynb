{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "filtered_bh.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNBYqKKQrKmsh2Xa//v/dXB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/22222bh/SketchTogether/blob/user%2FByeonghyeon/filtered_bh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSSGp-AdN1hi"
      },
      "source": [
        "초기 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nwTUYKTN0SA",
        "outputId": "90130835-bd3a-46ba-f25f-8e1c9834d295"
      },
      "source": [
        "!pip install efficientnet_pytorch\n",
        "!pip install umap-learn\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting efficientnet_pytorch\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet_pytorch) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch) (3.7.4.3)\n",
            "Building wheels for collected packages: efficientnet-pytorch\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=724ad5f8554da29ca159d0b58987768f7978f1ff1fd4869ede912ac3cabfa76f\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/cc/b2/49e74588263573ff778da58cc99b9c6349b496636a7e165be6\n",
            "Successfully built efficientnet-pytorch\n",
            "Installing collected packages: efficientnet-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1\n",
            "Collecting umap-learn\n",
            "  Downloading umap-learn-0.5.1.tar.gz (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 8.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.4.1)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.51.2)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.4.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 30.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (57.2.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (0.34.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pynndescent>=0.5->umap-learn) (1.0.1)\n",
            "Building wheels for collected packages: umap-learn, pynndescent\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.1-py3-none-any.whl size=76566 sha256=4ce5f76800b52b9bbd7fed0f2aa251009f29afb9cc2a891ea4e122e927b01685\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/e7/bb/347dc0e510803d7116a13d592b10cc68262da56a8eec4dd72f\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.4-py3-none-any.whl size=52372 sha256=a2e9a16f79b8ff23b0fbba0760cf5383f391020b184d157738da54a10423c840\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/5b/62/3401692ddad12324249c774c4b15ccb046946021e2b581c043\n",
            "Successfully built umap-learn pynndescent\n",
            "Installing collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.4 umap-learn-0.5.1\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_vfG8EQN1VP"
      },
      "source": [
        "from efficientnet_pytorch import EfficientNet\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "# from keras.applications.vgg16 import VGG16 \n",
        "# from keras.models import Model\n",
        "# from keras.applications.vgg16 import preprocess_input \n",
        "import os\n",
        "import logging\n",
        "import torch\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Pyjvf2NODmd"
      },
      "source": [
        "filename = 'filtered_10'\n",
        "data_path = '/content/drive/MyDrive/Sketch_RNN/Dataset/Filtered_img/'\n",
        "cluster_list_path = '/content/drive/MyDrive/Sketch_RNN/Dataset/Filtered_img/cluster_list.txt'\n",
        "feat_path = \"/content/drive/MyDrive/skku_Clustering/features_hw_\" + filename + \".npy\"\n",
        "filename_path = \"/content/drive/MyDrive/skku_Clustering/filenames_hw_\" + filename + \".npy\"\n",
        "save_path = '/content/drive/MyDrive/skku_Clustering/Result/'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JETaCQloN-s7"
      },
      "source": [
        "os.chdir(data_path)\n",
        "data_list = []\n",
        "with os.scandir(data_path) as files:\n",
        "    for file in files:\n",
        "        if not file.name.endswith('.txt'):\n",
        "            data_list.append(file.name)\n",
        "\n",
        "cluster_list = open(cluster_list_path , 'r').read().split('\\n') \n",
        "\n",
        "#transform\n",
        "tfms = transforms.Compose([transforms.Resize(224), transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),])\n",
        "\n",
        "def feature_extraction(image, model):\n",
        "    img = tfms(image.convert(\"RGB\")).unsqueeze(0)\n",
        "    features = model.extract_features(img)\n",
        "    return features\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSJIJ0AbOJIR"
      },
      "source": [
        "from torch import nn\n",
        "model = EfficientNet.from_pretrained('efficientnet-b0')\n",
        "data = {}\n",
        "avg_pooling = nn.AdaptiveAvgPool2d(1)\n",
        "for folder in data_list:\n",
        "    image_list = os.listdir(data_path + folder)\n",
        "    print(image_list)\n",
        "    print(folder)\n",
        "    for i, image_name in enumerate(image_list):\n",
        "      # try:\n",
        "        image = Image.open(data_path + folder +'/'+ image_name)\n",
        "        feat = feature_extraction(image, model)\n",
        "        feat = avg_pooling(feat)\n",
        "        feat = feat.detach().numpy().reshape(-1)\n",
        "        name = folder + '_' + str(i)\n",
        "        data[name] = (feat)\n",
        "\n",
        "np.save(feat_path, np.array(list(data.values())))\n",
        "np.save(filename_path, np.array(list(data.keys())))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Co4TKVlOJ5P"
      },
      "source": [
        "feat = np.load(feat_path)\n",
        "#TSNE\n",
        "from sklearn.manifold import TSNE\n",
        "tsne = TSNE(n_components=2, init='pca', random_state=0)\n",
        "feat = tsne.fit_transform(feat)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS-USqfMVjQ9"
      },
      "source": [
        "dbscan 관련 파라미터는 여기서 수정하면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpUIT0MBOMLX",
        "outputId": "89a66312-d9d8-4a71-f61f-63eac0dd4759"
      },
      "source": [
        "_eps = 1.7\n",
        "_ms = 6\n",
        "\n",
        "\n",
        "def clustering_method(cmd, cluster_num, input):\n",
        "  if cmd == 'kmeans':\n",
        "    from sklearn.cluster import KMeans\n",
        "    kmeans = KMeans(n_clusters=cluster_num, n_init = 20, n_jobs=-1, random_state = 0)\n",
        "    kmeans.fit(input)\n",
        "    return kmeans.labels_\n",
        "\n",
        "  elif cmd == 'minibatch':\n",
        "    from sklearn.cluster import MiniBatchKMeans\n",
        "    mb_kmeans = MiniBatchKMeans(n_clusters=cluster_num,random_state=0, batch_size=6)\n",
        "    mb_kmeans.fit(input)\n",
        "    return mb_kmeans.labels_\n",
        "\n",
        "  elif cmd == 'dbscan':\n",
        "    from sklearn.cluster import DBSCAN\n",
        "    dbscan = DBSCAN(eps=_eps, min_samples=_ms) # eps 1.8이면 63프로\n",
        "    dbscan.fit(input)\n",
        "    return dbscan.labels_\n",
        "  \n",
        "  elif cmd == 'optics':\n",
        "    from sklearn.cluster import OPTICS, cluster_optics_dbscan\n",
        "    optics = OPTICS(min_samples = 5)\n",
        "    optics.fit(input)\n",
        "    return optics.labels_\n",
        "\n",
        "labels = clustering_method('dbscan', len(cluster_list), feat)\n",
        "\n",
        "filenames = np.load(filename_path)\n",
        "groups = {}\n",
        "# holds the cluster id and the images { id: [images] }\n",
        "i = 0\n",
        "for f, cluster in zip(filenames, labels):\n",
        "    if cluster not in groups.keys():\n",
        "        groups[cluster] = []\n",
        "        groups[cluster].append(f)\n",
        "    else:\n",
        "        groups[cluster].append(f)\n",
        "\n",
        "\n",
        "#make cluster_dict for calculate acc\n",
        "cluster_dict = {}\n",
        "for cluster in groups:\n",
        "  image_count = []\n",
        "  image_name = []\n",
        "  for image in groups[cluster]:\n",
        "    image_name.append(image.split('_')[0])\n",
        "  for name in cluster_list:\n",
        "    image_count.append(image_name.count(name))\n",
        "  cluster_dict[cluster] = cluster_list[image_count.index(max(image_count))]\n",
        "print(len(cluster_dict))\n",
        "print(cluster_dict)\n",
        "\n",
        "#Acc\n",
        "from sklearn.metrics import f1_score\n",
        "pred = []\n",
        "gt = []    \n",
        "for cluster in groups:\n",
        "    for food in groups[cluster]:\n",
        "        pred.append(cluster_dict[cluster])\n",
        "        # gt.append(food.split('_')[3].split('-')[0])\n",
        "        gt.append(food.split('_')[0])\n",
        "\n",
        "acc = str(round(f1_score(gt, pred,average='micro') * 100, 2))\n",
        "print(\"F1 ACC: \" + acc)\n",
        "save_path = '/content/drive/MyDrive/skku_Clustering/Result_' + str(_eps) + '_' + str(_ms) + '_' + acc + '%_' + str(len(cluster_dict)) + '/'\n",
        "print(save_path)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27\n",
            "{1: 'ear', 2: 'ear', 0: 'ear', -1: 'dog', 3: 'ear', 4: 'circle', 6: 'cat', 5: 'dog', 7: 'cat', 8: 'cat', 11: 'dog', 9: 'dog', 10: 'dog', 23: 'eye', 12: 'nose', 19: 'triangle', 13: 'nose', 14: 'triangle', 15: 'moustache', 16: 'moustache', 17: 'cloud', 18: 'triangle', 24: 'mouth', 20: 'triangle', 21: 'cloud', 22: 'eye', 25: 'mouth'}\n",
            "F1 ACC: 68.2\n",
            "/content/drive/MyDrive/skku_Clustering/Result_1.7_6_68.2%_27/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHTbce4fOYBl",
        "outputId": "45ed6897-a4ad-443c-c674-a4ee6160902a"
      },
      "source": [
        "# Save img\n",
        "if not os.path.isdir(save_path):\n",
        "  print('create dir')\n",
        "  os.makedirs(save_path)\n",
        "from PIL import Image        \n",
        "for cluster in groups:\n",
        "    for image in groups[cluster]:\n",
        "        path = save_path + str(cluster) + '_' + str(cluster_dict[cluster]) + '/'\n",
        "        if os.path.isdir(path) == False:\n",
        "            os.makedirs(path)\n",
        "        folder_name = image.split('_')[0] + '/'\n",
        "        img = Image.open(data_path + folder_name + image + '.jpg')\n",
        "        img.save(path + image.split('/')[-1] + '.jpg')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "create dir\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}